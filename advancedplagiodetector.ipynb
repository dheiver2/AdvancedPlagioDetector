{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nfrom typing import List, Dict, Tuple, Set\nfrom dataclasses import dataclass\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\nimport json\nfrom datetime import datetime\nimport warnings\nfrom difflib import SequenceMatcher\n\n# Suprime avisos específicos\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n@dataclass\nclass AnaliseSemantica:\n    \"\"\"Armazena resultados da análise semântica\"\"\"\n    similaridade: float\n    trechos_similares: List[Tuple[str, str, float]]\n    palavras_chave_comuns: List[str]\n    contexto_similar: bool\n    nivel_similaridade: str  # 'Alto', 'Médio', 'Baixo'\n\n@dataclass\nclass AnaliseSintatica:\n    \"\"\"Armazena resultados da análise sintática\"\"\"\n    similaridade: float\n    padrao_estrutural: float\n    sequencias_identicas: List[str]\n    alteracoes_detectadas: List[Tuple[str, str]]\n    nivel_similaridade: str  # 'Alto', 'Médio', 'Baixo'\n\n@dataclass\nclass PlagiarismReport:\n    \"\"\"Classe para armazenar resultados detalhados da análise de plágio.\"\"\"\n    texto_original: str\n    texto_suspeito: str\n    analise_semantica: AnaliseSemantica\n    analise_sintatica: AnaliseSintatica\n    plagio_detectado: bool\n    data_analise: str\n    metadados: Dict\n\nclass EnhancedPlagioDetector:\n    def __init__(self, \n                limiar_sintatico: float = 0.7,\n                limiar_semantico: float = 0.7,\n                tamanho_minimo_trecho: int = 10):\n        \"\"\"\n        Inicializa o detector de plágio avançado.\n        \n        Args:\n            limiar_sintatico: Limiar para similaridade sintática\n            limiar_semantico: Limiar para similaridade semântica\n            tamanho_minimo_trecho: Número mínimo de palavras para considerar um trecho\n        \"\"\"\n        # Configuração do NLTK\n        try:\n            nltk.data.find('tokenizers/punkt')\n            nltk.data.find('corpora/stopwords')\n        except LookupError:\n            print(\"Baixando recursos necessários do NLTK...\")\n            nltk.download('punkt', quiet=True)\n            nltk.download('stopwords', quiet=True)\n\n        self.vectorizador_tfidf = TfidfVectorizer(\n            ngram_range=(1, 3),\n            stop_words=set(stopwords.words('portuguese'))\n        )\n        self.limiar_sintatico = limiar_sintatico\n        self.limiar_semantico = limiar_semantico\n        self.tamanho_minimo_trecho = tamanho_minimo_trecho\n        self.historico_analises = []\n        self.stopwords = set(stopwords.words('portuguese'))\n\n    def preprocessar_texto(self, texto: str) -> str:\n        \"\"\"\n        Preprocessa o texto aplicando várias transformações.\n        \"\"\"\n        if not isinstance(texto, str):\n            raise ValueError(\"O texto deve ser uma string\")\n            \n        # Remove caracteres especiais mantendo espaços\n        texto = re.sub(r'[^\\w\\s]', '', texto)\n        # Converte para minúsculas\n        texto = texto.lower()\n        # Remove espaços extras\n        texto = ' '.join(texto.split())\n        return texto\n\n    def calcular_similaridade_sequencia(self, texto1: str, texto2: str) -> float:\n        \"\"\"\n        Calcula a similaridade entre duas strings usando SequenceMatcher.\n        \"\"\"\n        return SequenceMatcher(None, texto1, texto2).ratio()\n\n    def calcular_similaridade_tfidf(self, texto1: str, texto2: str) -> float:\n        \"\"\"\n        Calcula similaridade usando TF-IDF e similaridade do cosseno.\n        \"\"\"\n        if not texto1 or not texto2:\n            return 0.0\n            \n        textos = [texto1, texto2]\n        try:\n            tfidf_matriz = self.vectorizador_tfidf.fit_transform(textos)\n            similaridade = cosine_similarity(tfidf_matriz[0:1], tfidf_matriz[1:2])\n            return float(similaridade[0][0])\n        except Exception as e:\n            print(f\"Erro ao calcular similaridade TF-IDF: {e}\")\n            return 0.0\n\n    def dividir_em_trechos(self, texto: str) -> List[str]:\n        \"\"\"\n        Divide o texto em trechos menores para análise detalhada.\n        \"\"\"\n        try:\n            sentencas = sent_tokenize(texto)\n            trechos = []\n            trecho_atual = \"\"\n            \n            for sentenca in sentencas:\n                palavras_atual = len(trecho_atual.split())\n                palavras_sentenca = len(sentenca.split())\n                \n                if palavras_atual + palavras_sentenca <= self.tamanho_minimo_trecho:\n                    trecho_atual += \" \" + sentenca if trecho_atual else sentenca\n                else:\n                    if trecho_atual:\n                        trechos.append(trecho_atual.strip())\n                    trecho_atual = sentenca\n                    \n            if trecho_atual:\n                trechos.append(trecho_atual.strip())\n                \n            return trechos\n        except Exception as e:\n            print(f\"Erro ao dividir texto em trechos: {e}\")\n            return [texto]\n\n    def encontrar_trechos_similares(self, texto1: str, texto2: str) -> List[Tuple[str, str, float]]:\n        \"\"\"\n        Encontra pares de trechos similares entre os dois textos.\n        \"\"\"\n        trechos1 = self.dividir_em_trechos(texto1)\n        trechos2 = self.dividir_em_trechos(texto2)\n        \n        trechos_similares = []\n        \n        for t1 in trechos1:\n            for t2 in trechos2:\n                try:\n                    similaridade = self.calcular_similaridade_sequencia(t1, t2)\n                    if similaridade >= self.limiar_semantico:\n                        trechos_similares.append((t1, t2, similaridade))\n                except Exception as e:\n                    print(f\"Erro ao comparar trechos: {e}\")\n                    continue\n                    \n        return sorted(trechos_similares, key=lambda x: x[2], reverse=True)\n\n    def encontrar_sequencias_identicas(self, texto1: str, texto2: str) -> List[str]:\n        \"\"\"\n        Encontra sequências de palavras idênticas entre os textos.\n        \"\"\"\n        words1 = texto1.split()\n        words2 = texto2.split()\n        sequencias = []\n        \n        for i in range(len(words1)):\n            for j in range(len(words2)):\n                k = 0\n                while (i + k < len(words1) and j + k < len(words2) and \n                       words1[i + k] == words2[j + k]):\n                    k += 1\n                if k >= 3:  # Sequências de 3 ou mais palavras\n                    sequencia = ' '.join(words1[i:i + k])\n                    if not all(word in self.stopwords for word in sequencia.split()):\n                        sequencias.append(sequencia)\n        \n        return list(set(sequencias))\n\n    def detectar_alteracoes(self, texto1: str, texto2: str) -> List[Tuple[str, str]]:\n        \"\"\"\n        Detecta alterações simples como substituições de palavras.\n        \"\"\"\n        palavras1 = word_tokenize(texto1)\n        palavras2 = word_tokenize(texto2)\n        alteracoes = []\n        \n        for p1, p2 in zip(palavras1, palavras2):\n            if p1 != p2 and len(p1) > 3 and len(p2) > 3:\n                if p1 not in self.stopwords and p2 not in self.stopwords:\n                    similaridade = self.calcular_similaridade_sequencia(p1, p2)\n                    if similaridade > 0.5:  # Palavras similares\n                        alteracoes.append((p1, p2))\n        \n        return alteracoes\n\n    def calcular_padrao_estrutural(self, texto1: str, texto2: str) -> float:\n        \"\"\"\n        Calcula similaridade estrutural entre os textos.\n        \"\"\"\n        sent1 = sent_tokenize(texto1)\n        sent2 = sent_tokenize(texto2)\n        \n        # Compara número de sentenças e comprimento médio\n        len_ratio = min(len(sent1), len(sent2)) / max(len(sent1), len(sent2))\n        \n        avg_len1 = np.mean([len(s.split()) for s in sent1])\n        avg_len2 = np.mean([len(s.split()) for s in sent2])\n        \n        len_similarity = 1 - abs(avg_len1 - avg_len2) / max(avg_len1, avg_len2)\n        \n        return (len_ratio + len_similarity) / 2\n\n    def identificar_palavras_chave_comuns(self, texto1: str, texto2: str) -> List[str]:\n        \"\"\"\n        Identifica palavras-chave importantes que aparecem em ambos os textos.\n        \"\"\"\n        def get_keywords(texto: str) -> Set[str]:\n            words = word_tokenize(texto.lower())\n            return {w for w in words if w not in self.stopwords and len(w) > 3}\n        \n        keywords1 = get_keywords(texto1)\n        keywords2 = get_keywords(texto2)\n        \n        return sorted(list(keywords1.intersection(keywords2)))\n\n    def analisar_contexto(self, texto1: str, texto2: str) -> bool:\n        \"\"\"\n        Analisa se os textos compartilham o mesmo contexto/tema.\n        \"\"\"\n        keywords1 = set(self.identificar_palavras_chave_comuns(texto1, texto1))\n        keywords2 = set(self.identificar_palavras_chave_comuns(texto2, texto2))\n        \n        overlap = len(keywords1.intersection(keywords2))\n        total = len(keywords1.union(keywords2))\n        \n        return (overlap / total if total > 0 else 0) > 0.3\n\n    def realizar_analise_sintatica(self, texto1: str, texto2: str) -> AnaliseSintatica:\n        \"\"\"\n        Realiza análise sintática detalhada entre dois textos.\n        \"\"\"\n        similaridade = self.calcular_similaridade_tfidf(texto1, texto2)\n        sequencias = self.encontrar_sequencias_identicas(texto1, texto2)\n        alteracoes = self.detectar_alteracoes(texto1, texto2)\n        padrao = self.calcular_padrao_estrutural(texto1, texto2)\n        \n        if similaridade >= 0.8:\n            nivel = \"Alto\"\n        elif similaridade >= 0.5:\n            nivel = \"Médio\"\n        else:\n            nivel = \"Baixo\"\n            \n        return AnaliseSintatica(\n            similaridade=similaridade,\n            padrao_estrutural=padrao,\n            sequencias_identicas=sequencias,\n            alteracoes_detectadas=alteracoes,\n            nivel_similaridade=nivel\n        )\n\n    def realizar_analise_semantica(self, texto1: str, texto2: str) -> AnaliseSemantica:\n        \"\"\"\n        Realiza análise semântica detalhada entre dois textos.\n        \"\"\"\n        similaridade = self.calcular_similaridade_sequencia(texto1, texto2)\n        trechos = self.encontrar_trechos_similares(texto1, texto2)\n        palavras_chave = self.identificar_palavras_chave_comuns(texto1, texto2)\n        contexto_similar = self.analisar_contexto(texto1, texto2)\n        \n        if similaridade >= 0.8:\n            nivel = \"Alto\"\n        elif similaridade >= 0.5:\n            nivel = \"Médio\"\n        else:\n            nivel = \"Baixo\"\n            \n        return AnaliseSemantica(\n            similaridade=similaridade,\n            trechos_similares=trechos,\n            palavras_chave_comuns=palavras_chave,\n            contexto_similar=contexto_similar,\n            nivel_similaridade=nivel\n        )\n\n    def verificar_plagio(self, texto1: str, texto2: str) -> PlagiarismReport:\n        \"\"\"\n        Realiza uma análise completa de plágio com relatórios separados.\n        \"\"\"\n        if not isinstance(texto1, str) or not isinstance(texto2, str):\n            raise ValueError(\"Ambos os textos devem ser strings\")\n            \n        if not texto1.strip() or not texto2.strip():\n            raise ValueError(\"Os textos não podem estar vazios\")\n            \n        texto1_prep = self.preprocessar_texto(texto1)\n        texto2_prep = self.preprocessar_texto(texto2)\n        \n        analise_sintatica = self.realizar_analise_sintatica(texto1_prep, texto2_prep)\n        analise_semantica = self.realizar_analise_semantica(texto1_prep, texto2_prep)\n        \n        plagio_detectado = (\n            analise_sintatica.similaridade >= self.limiar_sintatico or\n            analise_semantica.similaridade >= self.limiar_semantico\n        )\n        \n        relatorio = PlagiarismReport(\n            texto_original=texto1,\n            texto_suspeito=texto2,\n            analise_semantica=analise_semantica,\n            analise_sintatica=analise_sintatica,\n            plagio_detectado=plagio_detectado,\n            data_analise=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            metadados={\n                \"tamanho_texto1\": len(texto1.split()),\n                \"tamanho_texto2\": len(texto2.split())\n            }\n        )\n        \n        self.historico_analises.append(relatorio)\n        return relatorio\n\n\n    def apresentar_resultados(self, relatorio: PlagiarismReport) -> None:\n        \"\"\"\n        Apresenta os resultados detalhados da análise.\n        \"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"RELATÓRIO DETALHADO DE ANÁLISE DE PLÁGIO\")\n        print(\"=\"*60)\n        \n        print(\"\\nRESULTADO GERAL:\")\n        print(f\"Data da análise: {relatorio.data_analise}\")\n        print(f\"Plágio detectado: {'SIM' if relatorio.plagio_detectado else 'NÃO'}\")\n        \n        print(\"\\nANÁLISE SINTÁTICA:\")\n        print(f\"Nível de Similaridade: {relatorio.analise_sintatica.nivel_similaridade}\")\n        print(f\"Similaridade TF-IDF: {relatorio.analise_sintatica.similaridade:.2%}\")\n        print(f\"Padrão Estrutural: {relatorio.analise_sintatica.padrao_estrutural:.2%}\")\n        \n        if relatorio.analise_sintatica.sequencias_identicas:\n            print(\"\\nSequências Idênticas Encontradas:\")\n            for seq in relatorio.analise_sintatica.sequencias_identicas[:3]:\n                print(f\"- {seq}\")\n                \n        if relatorio.analise_sintatica.alteracoes_detectadas:\n            print(\"\\nAlterações Detectadas:\")\n            for original, alterado in relatorio.analise_sintatica.alteracoes_detectadas[:3]:\n                print(f\"- {original} → {alterado}\")\n        \n        print(\"\\nANÁLISE SEMÂNTICA:\")\n        print(f\"Nível de Similaridade: {relatorio.analise_semantica.nivel_similaridade}\")\n        print(f\"Similaridade de Sequência: {relatorio.analise_semantica.similaridade:.2%}\")\n        print(f\"Contexto Similar: {'Sim' if relatorio.analise_semantica.contexto_similar else 'Não'}\")\n        \n        if relatorio.analise_semantica.palavras_chave_comuns:\n            print(\"\\nPalavras-chave Comuns:\")\n            print(\", \".join(relatorio.analise_semantica.palavras_chave_comuns[:10]))\n        \n        if relatorio.analise_semantica.trechos_similares:\n            print(\"\\nTrechos Semanticamente Similares:\")\n            for i, (original, suspeito, similaridade) in enumerate(\n                relatorio.analise_semantica.trechos_similares[:2], 1):\n                print(f\"\\nPar #{i} (Similaridade: {similaridade:.2%})\")\n                print(f\"Original : {original}\")\n                print(f\"Suspeito: {suspeito}\")\n        \n        print(\"\\nMETADADOS:\")\n        print(f\"Tamanho do texto original: {relatorio.metadados['tamanho_texto1']} palavras\")\n        print(f\"Tamanho do texto suspeito: {relatorio.metadados['tamanho_texto2']} palavras\")\n        \n        print(\"\\n\" + \"=\"*60 + \"\\n\")\n\ndef executar_exemplos():\n    \"\"\"\n    Executa exemplos de uso do detector de plágio.\n    \"\"\"\n    detector = EnhancedPlagioDetector(\n        limiar_sintatico=0.6,\n        limiar_semantico=0.6,\n        tamanho_minimo_trecho=8\n    )\n\n    # Exemplo 1: Plágio direto com pequenas alterações\n    texto_original_1 = \"\"\"\n    O aquecimento global é um fenômeno climático que indica o aumento da \n    temperatura média da Terra ao longo das últimas décadas. Este processo \n    está diretamente relacionado com a intensificação do efeito estufa, \n    causado principalmente pela emissão de gases poluentes na atmosfera.\n    Os principais gases causadores do efeito estufa são o dióxido de \n    carbono, o metano e o óxido nitroso.\n    \"\"\"\n    \n    texto_suspeito_1 = \"\"\"\n    O aquecimento global é um fenômeno climático que mostra o aumento da \n    temperatura média do planeta Terra nas últimas décadas. Este processo \n    está diretamente ligado com a intensificação do efeito estufa, \n    causado principalmente pela liberação de gases poluentes na atmosfera.\n    Os principais gases que causam o efeito estufa são o dióxido de \n    carbono, o metano e o óxido nitroso.\n    \"\"\"\n\n    # Exemplo 2: Plágio com paráfrase\n    texto_original_2 = \"\"\"\n    A Revolução Industrial, iniciada na Inglaterra no século XVIII,\n    foi um período de grandes transformações tecnológicas e sociais.\n    O processo de produção, antes artesanal e manual, passou a ser\n    realizado por máquinas, aumentando significativamente a produtividade.\n    Esta mudança teve profundo impacto na organização do trabalho e\n    na vida dos trabalhadores.\n    \"\"\"\n    \n    texto_suspeito_2 = \"\"\"\n    As transformações tecnológicas e sociais marcaram o período conhecido\n    como Revolução Industrial, que começou na Inglaterra durante o século XVIII.\n    A forma de produção se transformou completamente, substituindo o trabalho\n    manual e artesanal pela produção mecanizada, o que levou a um grande\n    aumento na produtividade. Essas alterações mudaram drasticamente como\n    o trabalho era organizado e afetaram profundamente a vida dos operários.\n    \"\"\"\n\n    # Exemplo 3: Textos diferentes sobre o mesmo tema\n    texto_original_3 = \"\"\"\n    A fotossíntese é um processo fundamental para a vida na Terra.\n    Durante este processo, as plantas utilizam a luz solar, água e\n    dióxido de carbono para produzir glicose e oxigênio. A clorofila,\n    pigmento verde presente nas folhas, é essencial para captar a\n    energia solar necessária para realizar este processo.\n    \"\"\"\n    \n    texto_suspeito_3 = \"\"\"\n    As plantas são organismos autótrofos que produzem seu próprio alimento\n    através da fotossíntese. Este processo bioquímico ocorre nas células\n    vegetais e requer luz solar, que é absorvida pela clorofila. Os\n    produtos finais são glicose, que serve como fonte de energia para\n    a planta, e oxigênio, que é liberado na atmosfera.\n    \"\"\"\n\n    exemplos = [\n        (texto_original_1, texto_suspeito_1, \"EXEMPLO 1: PLÁGIO DIRETO\"),\n        (texto_original_2, texto_suspeito_2, \"EXEMPLO 2: PLÁGIO COM PARÁFRASE\"),\n        (texto_original_3, texto_suspeito_3, \"EXEMPLO 3: TEXTOS RELACIONADOS\")\n    ]\n\n    for texto_original, texto_suspeito, titulo in exemplos:\n        print(f\"\\n{'='*70}\")\n        print(titulo)\n        print('='*70)\n        \n        relatorio = detector.verificar_plagio(texto_original, texto_suspeito)\n        detector.apresentar_resultados(relatorio)\n\nif __name__ == \"__main__\":\n    try:\n        executar_exemplos()\n    except Exception as e:\n        print(f\"Erro ao executar os exemplos: {str(e)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-06T09:32:21.067616Z","iopub.execute_input":"2024-11-06T09:32:21.068143Z","iopub.status.idle":"2024-11-06T09:32:21.153793Z","shell.execute_reply.started":"2024-11-06T09:32:21.068092Z","shell.execute_reply":"2024-11-06T09:32:21.152621Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n======================================================================\nEXEMPLO 1: PLÁGIO DIRETO\n======================================================================\nErro ao calcular similaridade TF-IDF: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'estes', 'deles', 'foi', 'estivéramos', 'seus', 'tenha', 'por', 'entre', 'estiveram', 'houvemos', 'tinha', 'ela', 'houveria', 'haver', 'e', 'seria', 'fôramos', 'tem', 'seremos', 'estivemos', 'forem', 'mas', 'esteve', 'as', 'estas', 'tenham', 'houveríamos', 'estivesse', 'muito', 'estivéssemos', 'me', 'ou', 'o', 'terão', 'sem', 'se', 'está', 'não', 'estávamos', 'houverem', 'para', 'havemos', 'mesmo', 'quando', 'somos', 'houveremos', 'houvermos', 'tém', 'tivemos', 'teríamos', 'lhes', 'aquela', 'estiver', 'nós', 'que', 'fomos', 'hão', 'tinham', 'estar', 'uma', 'tiver', 'aos', 'nossos', 'nos', 'só', 'esteja', 'teriam', 'tivéramos', 'isto', 'estão', 'haja', 'houvéssemos', 'essa', 'tivessem', 'seriam', 'te', 'num', 'hajamos', 'temos', 'às', 'era', 'houveram', 'estivessem', 'você', 'foram', 'essas', 'aqueles', 'for', 'tivermos', 'tua', 'dela', 'esses', 'estivermos', 'sejamos', 'serão', 'meu', 'tive', 'houverei', 'tivera', 'hajam', 'são', 'pela', 'estive', 'tiveram', 'no', 'na', 'até', 'em', 'éramos', 'houvessem', 'teu', 'depois', 'nossas', 'houver', 'fosse', 'numa', 'quem', 'será', 'terei', 'dos', 'os', 'teve', 'houveriam', 'seríamos', 'aquele', 'elas', 'seja', 'tivesse', 'pelos', 'serei', 'estamos', 'isso', 'sou', 'das', 'esta', 'do', 'houvera', 'estavam', 'houvesse', 'delas', 'ao', 'ser', 'tu', 'houvéramos', 'eu', 'como', 'eram', 'nas', 'tivéssemos', 'esse', 'houverão', 'estou', 'minhas', 'fora', 'tiverem', 'estava', 'sejam', 'ele', 'aquilo', 'também', 'a', 'de', 'houve', 'eles', 'estejamos', 'nem', 'sua', 'lhe', 'dele', 'hei', 'há', 'nossa', 'qual', 'seu', 'pelas', 'terá', 'com', 'teria', 'vos', 'fôssemos', 'formos', 'estiverem', 'teus', 'aquelas', 'houverá', 'teremos', 'fossem', 'tenho', 'meus', 'este', 'vocês', 'pelo', 'um', 'da', 'à', 'minha', 'estejam', 'mais', 'nosso', 'é', 'tenhamos', 'fui', 'tínhamos', 'já', 'estivera', 'tuas', 'suas'} instead.\n\n============================================================\nRELATÓRIO DETALHADO DE ANÁLISE DE PLÁGIO\n============================================================\n\nRESULTADO GERAL:\nData da análise: 2024-11-06 09:32:21\nPlágio detectado: SIM\n\nANÁLISE SINTÁTICA:\nNível de Similaridade: Baixo\nSimilaridade TF-IDF: 0.00%\nPadrão Estrutural: 100.00%\n\nSequências Idênticas Encontradas:\n- carbono o metano e o óxido nitroso\n- aquecimento global é um fenômeno climático que\n- poluentes na atmosfera os principais gases\n\nAlterações Detectadas:\n- causadores → causam\n\nANÁLISE SEMÂNTICA:\nNível de Similaridade: Médio\nSimilaridade de Sequência: 74.49%\nContexto Similar: Sim\n\nPalavras-chave Comuns:\naquecimento, atmosfera, aumento, carbono, causado, climático, diretamente, dióxido, décadas, efeito\n\nTrechos Semanticamente Similares:\n\nPar #1 (Similaridade: 74.49%)\nOriginal : o aquecimento global é um fenômeno climático que indica o aumento da temperatura média da terra ao longo das últimas décadas este processo está diretamente relacionado com a intensificação do efeito estufa causado principalmente pela emissão de gases poluentes na atmosfera os principais gases causadores do efeito estufa são o dióxido de carbono o metano e o óxido nitroso\nSuspeito: o aquecimento global é um fenômeno climático que mostra o aumento da temperatura média do planeta terra nas últimas décadas este processo está diretamente ligado com a intensificação do efeito estufa causado principalmente pela liberação de gases poluentes na atmosfera os principais gases que causam o efeito estufa são o dióxido de carbono o metano e o óxido nitroso\n\nMETADADOS:\nTamanho do texto original: 59 palavras\nTamanho do texto suspeito: 59 palavras\n\n============================================================\n\n\n======================================================================\nEXEMPLO 2: PLÁGIO COM PARÁFRASE\n======================================================================\nErro ao calcular similaridade TF-IDF: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'estes', 'deles', 'foi', 'estivéramos', 'seus', 'tenha', 'por', 'entre', 'estiveram', 'houvemos', 'tinha', 'ela', 'houveria', 'haver', 'e', 'seria', 'fôramos', 'tem', 'seremos', 'estivemos', 'forem', 'mas', 'esteve', 'as', 'estas', 'tenham', 'houveríamos', 'estivesse', 'muito', 'estivéssemos', 'me', 'ou', 'o', 'terão', 'sem', 'se', 'está', 'não', 'estávamos', 'houverem', 'para', 'havemos', 'mesmo', 'quando', 'somos', 'houveremos', 'houvermos', 'tém', 'tivemos', 'teríamos', 'lhes', 'aquela', 'estiver', 'nós', 'que', 'fomos', 'hão', 'tinham', 'estar', 'uma', 'tiver', 'aos', 'nossos', 'nos', 'só', 'esteja', 'teriam', 'tivéramos', 'isto', 'estão', 'haja', 'houvéssemos', 'essa', 'tivessem', 'seriam', 'te', 'num', 'hajamos', 'temos', 'às', 'era', 'houveram', 'estivessem', 'você', 'foram', 'essas', 'aqueles', 'for', 'tivermos', 'tua', 'dela', 'esses', 'estivermos', 'sejamos', 'serão', 'meu', 'tive', 'houverei', 'tivera', 'hajam', 'são', 'pela', 'estive', 'tiveram', 'no', 'na', 'até', 'em', 'éramos', 'houvessem', 'teu', 'depois', 'nossas', 'houver', 'fosse', 'numa', 'quem', 'será', 'terei', 'dos', 'os', 'teve', 'houveriam', 'seríamos', 'aquele', 'elas', 'seja', 'tivesse', 'pelos', 'serei', 'estamos', 'isso', 'sou', 'das', 'esta', 'do', 'houvera', 'estavam', 'houvesse', 'delas', 'ao', 'ser', 'tu', 'houvéramos', 'eu', 'como', 'eram', 'nas', 'tivéssemos', 'esse', 'houverão', 'estou', 'minhas', 'fora', 'tiverem', 'estava', 'sejam', 'ele', 'aquilo', 'também', 'a', 'de', 'houve', 'eles', 'estejamos', 'nem', 'sua', 'lhe', 'dele', 'hei', 'há', 'nossa', 'qual', 'seu', 'pelas', 'terá', 'com', 'teria', 'vos', 'fôssemos', 'formos', 'estiverem', 'teus', 'aquelas', 'houverá', 'teremos', 'fossem', 'tenho', 'meus', 'este', 'vocês', 'pelo', 'um', 'da', 'à', 'minha', 'estejam', 'mais', 'nosso', 'é', 'tenhamos', 'fui', 'tínhamos', 'já', 'estivera', 'tuas', 'suas'} instead.\n\n============================================================\nRELATÓRIO DETALHADO DE ANÁLISE DE PLÁGIO\n============================================================\n\nRESULTADO GERAL:\nData da análise: 2024-11-06 09:32:21\nPlágio detectado: NÃO\n\nANÁLISE SINTÁTICA:\nNível de Similaridade: Baixo\nSimilaridade TF-IDF: 0.00%\nPadrão Estrutural: 90.98%\n\nSequências Idênticas Encontradas:\n- transformações tecnológicas e sociais\n- tecnológicas e sociais\n\nANÁLISE SEMÂNTICA:\nNível de Similaridade: Baixo\nSimilaridade de Sequência: 16.60%\nContexto Similar: Sim\n\nPalavras-chave Comuns:\nartesanal, industrial, inglaterra, manual, período, produtividade, produção, revolução, sociais, século\n\nMETADADOS:\nTamanho do texto original: 50 palavras\nTamanho do texto suspeito: 61 palavras\n\n============================================================\n\n\n======================================================================\nEXEMPLO 3: TEXTOS RELACIONADOS\n======================================================================\nErro ao calcular similaridade TF-IDF: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'estes', 'deles', 'foi', 'estivéramos', 'seus', 'tenha', 'por', 'entre', 'estiveram', 'houvemos', 'tinha', 'ela', 'houveria', 'haver', 'e', 'seria', 'fôramos', 'tem', 'seremos', 'estivemos', 'forem', 'mas', 'esteve', 'as', 'estas', 'tenham', 'houveríamos', 'estivesse', 'muito', 'estivéssemos', 'me', 'ou', 'o', 'terão', 'sem', 'se', 'está', 'não', 'estávamos', 'houverem', 'para', 'havemos', 'mesmo', 'quando', 'somos', 'houveremos', 'houvermos', 'tém', 'tivemos', 'teríamos', 'lhes', 'aquela', 'estiver', 'nós', 'que', 'fomos', 'hão', 'tinham', 'estar', 'uma', 'tiver', 'aos', 'nossos', 'nos', 'só', 'esteja', 'teriam', 'tivéramos', 'isto', 'estão', 'haja', 'houvéssemos', 'essa', 'tivessem', 'seriam', 'te', 'num', 'hajamos', 'temos', 'às', 'era', 'houveram', 'estivessem', 'você', 'foram', 'essas', 'aqueles', 'for', 'tivermos', 'tua', 'dela', 'esses', 'estivermos', 'sejamos', 'serão', 'meu', 'tive', 'houverei', 'tivera', 'hajam', 'são', 'pela', 'estive', 'tiveram', 'no', 'na', 'até', 'em', 'éramos', 'houvessem', 'teu', 'depois', 'nossas', 'houver', 'fosse', 'numa', 'quem', 'será', 'terei', 'dos', 'os', 'teve', 'houveriam', 'seríamos', 'aquele', 'elas', 'seja', 'tivesse', 'pelos', 'serei', 'estamos', 'isso', 'sou', 'das', 'esta', 'do', 'houvera', 'estavam', 'houvesse', 'delas', 'ao', 'ser', 'tu', 'houvéramos', 'eu', 'como', 'eram', 'nas', 'tivéssemos', 'esse', 'houverão', 'estou', 'minhas', 'fora', 'tiverem', 'estava', 'sejam', 'ele', 'aquilo', 'também', 'a', 'de', 'houve', 'eles', 'estejamos', 'nem', 'sua', 'lhe', 'dele', 'hei', 'há', 'nossa', 'qual', 'seu', 'pelas', 'terá', 'com', 'teria', 'vos', 'fôssemos', 'formos', 'estiverem', 'teus', 'aquelas', 'houverá', 'teremos', 'fossem', 'tenho', 'meus', 'este', 'vocês', 'pelo', 'um', 'da', 'à', 'minha', 'estejam', 'mais', 'nosso', 'é', 'tenhamos', 'fui', 'tínhamos', 'já', 'estivera', 'tuas', 'suas'} instead.\n\n============================================================\nRELATÓRIO DETALHADO DE ANÁLISE DE PLÁGIO\n============================================================\n\nRESULTADO GERAL:\nData da análise: 2024-11-06 09:32:21\nPlágio detectado: NÃO\n\nANÁLISE SINTÁTICA:\nNível de Similaridade: Baixo\nSimilaridade TF-IDF: 0.00%\nPadrão Estrutural: 99.00%\n\nANÁLISE SEMÂNTICA:\nNível de Similaridade: Baixo\nSimilaridade de Sequência: 7.18%\nContexto Similar: Não\n\nPalavras-chave Comuns:\nclorofila, energia, fotossíntese, glicose, oxigênio, plantas, processo, solar\n\nMETADADOS:\nTamanho do texto original: 49 palavras\nTamanho do texto suspeito: 50 palavras\n\n============================================================\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# import nltk\n# nltk.download('punkt')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T09:23:55.140471Z","iopub.execute_input":"2024-11-06T09:23:55.140923Z","iopub.status.idle":"2024-11-06T09:23:56.965075Z","shell.execute_reply.started":"2024-11-06T09:23:55.140883Z","shell.execute_reply":"2024-11-06T09:23:56.964022Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# !pip install sentence-transformers nltk tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T09:23:39.202877Z","iopub.execute_input":"2024-11-06T09:23:39.203415Z","iopub.status.idle":"2024-11-06T09:23:52.851925Z","shell.execute_reply.started":"2024-11-06T09:23:39.203371Z","shell.execute_reply":"2024-11-06T09:23:52.850466Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0+cpu)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.2.1\n","output_type":"stream"}]}]}